{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99111457",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "244652f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from deepagents import create_deep_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"mcp_zero_and_alita_rag\": {\n",
    "            \"url\": \"http://localhost:8000/mcp\",\n",
    "            \"transport\": \"streamable_http\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"gpt-oss:20b\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8786bb08",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc34359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How does Alita enable agents to create and deploy MCP tools if the agent deems that the tool does not exist?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  alita_documentation (8fa8565d-6f78-4eb2-8f63-10229df3ab17)\n",
      " Call ID: 8fa8565d-6f78-4eb2-8f63-10229df3ab17\n",
      "  Args:\n",
      "    query: How does Alita enable agents to create and deploy MCP tools if the agent deems that the tool does not exist?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: alita_documentation\n",
      "\n",
      "<think>\n",
      "Okay, let's tackle this query. The user is asking how Alita enables agents to create and deploy MCP tools when the tool doesn't exist. I need to go through the context provided and extract the relevant information.\n",
      "\n",
      "First, I remember from the context that Alita uses MCP Brainstorming, ScriptGeneratingTool, and CodeRunningTool. The MCP Brainstorming is part of the process to identify if existing tools are sufficient. If not, it suggests creating new tools. The ScriptGeneratingTool then generates the necessary code based on the task description and any information gathered from the web agent. The CodeRunningTool executes these scripts in isolated environments to test them. If successful, they're encapsulated as MCPs for reuse.\n",
      "\n",
      "Also, the web agent is involved in searching for existing tools or code snippets, like using GoogleSearchTool or GithubSearchTool. The Manager Agent coordinates all these steps, decomposing tasks into subtasks and delegating to the web agent or generating tools. The environment management part is important too, where dependencies are installed in isolated environments using Conda or pip. \n",
      "\n",
      "The MCP creation process is iterative, with feedback loops to refine the tools. If there's an error during execution, Alita can self-correct by adjusting the environment or the script. The MCPs are stored in an MCP Box for future use, allowing other agents to benefit from them. \n",
      "\n",
      "I need to make sure I cover all these components: the brainstorming phase, tool generation, environment setup, execution, validation, and reuse. Also, mention the collaboration between the Manager Agent and Web Agent, and how the system handles failures through recovery procedures. Highlight the self-evolving aspect and the use of MCPs for encapsulation and reuse.\n",
      "</think>\n",
      "\n",
      "Alita enables agents to create and deploy MCP tools when existing tools are insufficient through a structured, iterative process that leverages its core components and design principles. Here is a detailed breakdown of the mechanism:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Initial Capability Assessment via MCP Brainstorming**\n",
      "When an agent identifies a task requiring functionality not covered by existing tools, it initiates the **MCP Brainstorming** phase. This process involves:\n",
      "- **Task Analysis**: The agent evaluates the task's requirements and compares them against the current framework's capabilities.\n",
      "- **Gap Identification**: By analyzing the task description and the framework's existing tools, the agent detects functional gaps. For example, if the task requires extracting subtitles from a YouTube 360 VR video, the agent may recognize that no pre-existing tool exists for this specific purpose.\n",
      "- **Tool Specification Generation**: The agent generates a detailed specification for the required tool, outlining its purpose, inputs, outputs, and expected behavior. This specification serves as a blueprint for tool creation.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Web Agent-Driven Tool Discovery and Information Gathering**\n",
      "To address the identified gap, the **Web Agent** is deployed to search for relevant tools or code snippets. This involves:\n",
      "- **Open-Source Search**: The Web Agent uses tools like `GoogleSearchTool` and `GithubSearchTool` to query repositories (e.g., GitHub) for existing implementations. For instance, it might locate the `youtube-transcript-api` repository, which provides a Python library for extracting YouTube video transcripts.\n",
      "- **Code and Documentation Retrieval**: The Web Agent extracts relevant code snippets, documentation, and dependencies from the search results. This information is used to inform the tool's design and implementation.\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Script Generation via ScriptGeneratingTool**\n",
      "Once the Web Agent has gathered sufficient information, the **ScriptGeneratingTool** is activated to construct the tool. This tool:\n",
      "- **Synthesizes Code**: It generates a script based on the task requirements, the tool specification, and any code snippets retrieved by the Web Agent. For example, it might produce a Python script that uses the `youtube-transcript-api` to fetch video transcripts.\n",
      "- **Environment Setup**: The tool also generates environment configuration instructions (e.g., `conda create -n youtube_transcript` and `pip install youtube-transcript-api`) to ensure the script runs in an isolated, reproducible environment.\n",
      "- **Self-Contained Execution**: The generated script is designed to be self-contained, incorporating all necessary dependencies and logic to execute the task independently.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Virtual Environment Execution via CodeRunningTool**\n",
      "The **CodeRunningTool** validates the generated script by executing it in an isolated environment:\n",
      "- **Isolation and Reproducibility**: The script runs in a sandboxed environment (e.g., a Conda virtual environment) to prevent conflicts with the host system and ensure reproducibility.\n",
      "- **Error Detection and Correction**: If the script fails due to syntax errors, missing dependencies, or incompatible libraries, the CodeRunningTool logs the error and triggers Alita's automated recovery procedures. These may include:\n",
      "  - Relaxing version constraints on dependencies.\n",
      "  - Identifying minimal required dependencies to resolve the issue.\n",
      "  - Discarding the tool if recovery attempts fail and logging the failure for analysis.\n",
      "\n",
      "---\n",
      "\n",
      "### **5. MCP Encapsulation and Reusability**\n",
      "If the script executes successfully, the tool is **encapsulated as an MCP (Model Context Protocol)**:\n",
      "- **MCP Creation**: The tool is wrapped into an MCP, which standardizes how it interacts with LLMs and external systems. This MCP includes the script, its environment configuration, and metadata describing its purpose and usage.\n",
      "- **Storage in MCP Box**: The MCP is stored in the **MCP Box**, a centralized repository of reusable tools. This allows the tool to be shared with other agents or reused in future tasks without requiring repeated development.\n",
      "\n",
      "---\n",
      "\n",
      "### **6. Integration into the Agent's Workflow**\n",
      "The newly created MCP is integrated into the agent's workflow:\n",
      "- **Task-Specific Tooling**: The MCP is added to the agent's toolset, enabling it to handle the specific task (e.g., extracting subtitles from the YouTube video) efficiently.\n",
      "- **Self-Evolution**: The agent's **Manager Agent** continuously monitors the MCP's performance and may refine or expand it over time. For example, if the MCP is used in multiple tasks, it might be enhanced to handle variations of the same problem.\n",
      "\n",
      "---\n",
      "\n",
      "### **7. Collaboration Between Manager Agent and Web Agent**\n",
      "The **Manager Agent** orchestrates the entire process, while the **Web Agent** provides external information retrieval capabilities:\n",
      "- **Decomposition of Tasks**: The Manager Agent breaks down complex tasks into subtasks, delegating some to the Web Agent for external data retrieval (e.g., fetching code snippets) and others to the MCP creation pipeline.\n",
      "- **Dynamic Adaptation**: If the Web Agent identifies a more efficient tool or codebase, the Manager Agent can dynamically adjust the tool generation process to incorporate this information.\n",
      "\n",
      "---\n",
      "\n",
      "### **8. Environment Management for Isolation and Compatibility**\n",
      "Alita ensures compatibility and isolation through its **environment management system**:\n",
      "- **Dependency Parsing**: The system uses tools like `TextInspectorTool` to parse metadata (e.g., `requirements.txt`, `README.md`) from repositories or scripts, extracting dependencies and setup instructions.\n",
      "- **Isolated Execution Profiles**: A new Conda environment is created for each tool, with a unique name (e.g., derived from the task ID or repository path). Dependencies are installed using `conda install` or `pip install`, ensuring that each tool operates in its own isolated environment.\n",
      "- **Portability**: By avoiding containerization technologies, Alita maintains high compatibility across different systems while preserving the portability of its environments.\n",
      "\n",
      "---\n",
      "\n",
      "### **9. Iterative Refinement and Feedback Loops**\n",
      "Alita's process is iterative, allowing for continuous improvement:\n",
      "- **Feedback from Execution**: If a tool fails during execution, the Manager Agent logs the error and initiates recovery procedures. This feedback is used to refine the tool or adjust the environment configuration.\n",
      "- **Self-Correction**: The system automatically corrects errors by modifying the tool's script, environment setup, or dependencies, ensuring robustness and reliability.\n",
      "\n",
      "---\n",
      "\n",
      "### **10. Cross-Ecosystem Compatibility via MCPs**\n",
      "The use of MCPs ensures that tools created by Alita are compatible with diverse ecosystems:\n",
      "- **Standardized Interfaces**: MCPs provide a standardized way for agents to interact with external systems, enabling seamless integration with tools written in different languages or frameworks (e.g., Python, JavaScript).\n",
      "- **Reusability Across Agents**: MCPs generated by Alita can be shared with other agents, enhancing their capabilities without requiring them to reimplement the same functionality.\n",
      "\n",
      "---\n",
      "\n",
      "### **Summary**\n",
      "Alita enables agents to create and deploy MCP tools when existing tools are insufficient through a combination of **MCP Brainstorming**, **Web Agent-driven discovery**, **ScriptGeneratingTool**-based code synthesis, **CodeRunningTool**-mediated validation, and **environment management**. This process is orchestrated by the **Manager Agent**, which ensures the tools are self-contained, reusable, and compatible with diverse systems. By minimizing predefinition and maximizing self-evolution, Alita allows agents to dynamically adapt to new tasks, creating a self-reinforcing cycle of capability expansion.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "**Alita’s “tool‑creation pipeline”** lets an agent invent a new MCP (Model‑Context‑Protocol) tool on‑the‑fly when it discovers that no existing tool satisfies a task. The process is fully automated, iterative, and tightly coupled to the rest of the Alita ecosystem.\n",
      "\n",
      "| Step | What happens | Key components |\n",
      "|------|--------------|----------------|\n",
      "| **1. Gap detection (MCP Brainstorming)** | The agent (via the Manager Agent) decomposes the user request, compares the required capabilities against the current MCP library, and flags a missing function. | Manager Agent, MCP Brainstorming module |\n",
      "| **2. External knowledge gathering (Web Agent)** | The Web Agent searches the web (Google, GitHub, StackOverflow, etc.) for code snippets, libraries, or documentation that could satisfy the missing capability. | Web Agent, GoogleSearchTool, GithubSearchTool, TextInspectorTool |\n",
      "| **3. Specification & code synthesis (ScriptGeneratingTool)** | Using the gathered snippets and the task description, the ScriptGeneratingTool writes a self‑contained script that implements the new tool. It also generates an environment‑spec file (e.g., `requirements.txt` or Conda YAML). | ScriptGeneratingTool |\n",
      "| **4. Validation & execution (CodeRunningTool)** | The script is executed in a fresh, isolated environment (Conda or virtualenv). The CodeRunningTool monitors for errors, logs failures, and can trigger automatic retries or adjustments (e.g., relaxing version constraints). | CodeRunningTool |\n",
      "| **5. Encapsulation as an MCP** | If the script runs successfully, it is wrapped into an MCP: a standardized interface (input‑output schema, metadata, environment config) that any Alita agent can invoke. | MCP wrapper |\n",
      "| **6. Storage & reuse (MCP Box)** | The new MCP is stored in the MCP Box, a shared repository of reusable tools. Other agents can pull it from the box, reducing duplication of effort. | MCP Box |\n",
      "| **7. Integration & feedback** | The Manager Agent adds the MCP to the agent’s toolset, updates the task plan, and logs the creation. Future runs of the same or similar tasks will automatically use the new MCP. | Manager Agent, MCP Box |\n",
      "\n",
      "### How it works in practice\n",
      "\n",
      "1. **Agent identifies a missing tool** – e.g., “extract subtitles from a 360‑VR YouTube video.”  \n",
      "2. **Brainstorming** flags that no such MCP exists.  \n",
      "3. **Web Agent** searches GitHub, finds `youtube-transcript-api`, pulls the relevant code and docs.  \n",
      "4. **ScriptGeneratingTool** writes a Python script that calls the API, plus a `requirements.txt` with `youtube-transcript-api`.  \n",
      "5. **CodeRunningTool** spins up a new Conda env, installs the dependency, runs the script, and verifies the output.  \n",
      "6. **MCP wrapper** is created, exposing a simple `extract_subtitles(video_url)` interface.  \n",
      "7. The MCP is stored in the MCP Box, and the agent now has a ready‑to‑use tool for the current and future tasks.\n",
      "\n",
      "### Key design principles that make this possible\n",
      "\n",
      "- **Decoupled agents**: The Manager Agent orchestrates, the Web Agent fetches external data, and the Script/Code tools handle synthesis and validation.  \n",
      "- **Isolation**: Each new tool runs in its own Conda/venv, preventing dependency clashes.  \n",
      "- **Self‑repair**: If execution fails, the pipeline automatically retries with adjusted dependencies or logs the failure for human review.  \n",
      "- **Reusability**: MCPs are stored centrally, so the effort is amortized across all agents.  \n",
      "- **Iterative refinement**: Feedback from execution feeds back into the next brainstorming cycle, improving future tool creation.\n",
      "\n",
      "In short, Alita turns a “tool‑does‑not‑exist” signal into a fully automated, end‑to‑end pipeline that searches the web, synthesizes code, validates it, packages it as an MCP, and makes it available to all agents—effectively letting agents *create* the tools they need on demand.\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "react_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "async for chunk in react_agent.astream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"How does Alita enable agents to create and deploy MCP tools if the agent deems that the tool does not exist?\"}\n",
    "        ]\n",
    "    },\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    if \"messages\" in chunk:\n",
    "        chunk['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f547a1e",
   "metadata": {},
   "source": [
    "## Running our first deep agent without sub agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beef8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_deep_agent = create_deep_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    instructions=\"\"\"\n",
    "    You are an expert in the domain of MCP Zero and Alita. Your job is write a comprehensive report through\n",
    "    thorough research and analysis. \n",
    "    \n",
    "    Here are the instructions for writing the final report:\n",
    "\n",
    "    <report_instructions>\n",
    "\n",
    "    CRITICAL: Make sure the answer is written in the same language as the human messages! If you make a todo plan - you should note in the plan what language the report should be in so you dont forget!\n",
    "    Note: the language the report should be in is the language the QUESTION is in, not the language/country that the question is ABOUT.\n",
    "\n",
    "    Please create a detailed answer to the overall research brief that:\n",
    "    1. Is well-organized with proper headings (# for title, ## for sections, ### for subsections)\n",
    "    2. Includes specific facts and insights from the research\n",
    "    3. Provides a balanced, thorough analysis. Be as comprehensive as possible, and include all information that is relevant to the overall research question. People are using you for deep research and will expect detailed, comprehensive answers.\n",
    "    You can structure your report in a number of different ways. Here are some examples:\n",
    "\n",
    "    To answer a question that asks you to compare two things, you might structure your report like this:\n",
    "    1/ intro\n",
    "    2/ overview of topic A\n",
    "    3/ overview of topic B\n",
    "    4/ comparison between A and B\n",
    "    5/ conclusion\n",
    "\n",
    "    To answer a question that asks you to return a list of things, you might only need a single section which is the entire list.\n",
    "    1/ list of things or table of things\n",
    "    Or, you could choose to make each item in the list a separate section in the report. When asked for lists, you don't need an introduction or conclusion.\n",
    "    1/ item 1\n",
    "    2/ item 2\n",
    "    3/ item 3\n",
    "\n",
    "    To answer a question that asks you to summarize a topic, give a report, or give an overview, you might structure your report like this:\n",
    "    1/ overview of topic\n",
    "    2/ concept 1\n",
    "    3/ concept 2\n",
    "    4/ concept 3\n",
    "    5/ conclusion\n",
    "\n",
    "    If you think you can answer the question with a single section, you can do that too!\n",
    "    1/ answer\n",
    "\n",
    "    REMEMBER: Section is a VERY fluid and loose concept. You can structure your report however you think is best, including in ways that are not listed above!\n",
    "    Make sure that your sections are cohesive, and make sense for the reader.\n",
    "\n",
    "    For each section of the report, do the following:\n",
    "    - Use simple, clear language\n",
    "    - Use ## for section title (Markdown format) for each section of the report\n",
    "    - Do NOT ever refer to yourself as the writer of the report. This should be a professional report without any self-referential language. \n",
    "    - Do not say what you are doing in the report. Just write the report without any commentary from yourself.\n",
    "    - Each section should be as long as necessary to deeply answer the question with the information you have gathered. It is expected that sections will be fairly long and verbose. You are writing a deep research report, and users will expect a thorough answer.\n",
    "    - Use bullet points to list out information when appropriate, but by default, write in paragraph form.\n",
    "\n",
    "    REMEMBER:\n",
    "    The brief and research may be in English, but you need to translate this information to the right language when writing the final answer.\n",
    "    Make sure the final answer report is in the SAME language as the human messages in the message history.\n",
    "    </report_instructions>\n",
    "\n",
    "    You have access to a few tools. Namely:\n",
    "    ## `alita_documentation`\n",
    "    ## `mcp_zero_documentation`\n",
    "\n",
    "    Use these tools to get information about Alita and MCP Zero. \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b749f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for chunk in base_deep_agent.astream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"How does Alita enable agents to create and deploy MCP tools if the agent deems that the tool does not exist?\"}\n",
    "        ]\n",
    "    },\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    if \"messages\" in chunk:\n",
    "        chunk['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5fecf4",
   "metadata": {},
   "source": [
    "## Running with sub agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a604eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "alita_mcp_zero_prompt = \"\"\"\n",
    "You are an expert in all matters relating to MCP Zero and Alita. Use the tools you have \n",
    "available to answer the user's question. Do not answer the questions directly. Always use the tools.\n",
    "\n",
    "Write your answer in a report format with sections and subsections. Be crisp in your writing.\n",
    "\"\"\"\n",
    "\n",
    "alita_mcp_zero_sub_agent = {\n",
    "    \"name\": \"alita-mcp-zero-agent\",\n",
    "    \"description\": \"Used to answer questions about MCP Zero and Alita.\",\n",
    "    \"prompt\": alita_mcp_zero_prompt,\n",
    "    \"tools\": tools,\n",
    "    \"model_settings\": {\n",
    "        \"model\": \"ollama:qwen3:latest\",\n",
    "        \"temperature\":0,\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f106fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "critiquer_prompt = \"\"\"\n",
    "You are a dedicated editor tasked to critique a report. You can find the report at \n",
    "`final_report.md`. You can find the question/topic for this report at `question.txt`.\n",
    "The user may ask for specific areas to critique the report in. Respond to the user with\n",
    "a detailed critique of the report and suggest improvements. Be constructive in your critique.\n",
    "\n",
    "Do not write the `final_report.md` file yourself.\n",
    "\n",
    "Things to check:\n",
    "- Check that each section is appropriately named\n",
    "- Check that the report is written as you would find in an essay or a textbook - it should be text heavy, do not let it just be a list of bullet points!\n",
    "- Check that the report is comprehensive. If any paragraphs or sections are short, or missing important details, point it out.\n",
    "- Check that the article covers key areas of the industry, ensures overall understanding, and does not omit important parts.\n",
    "- Check that the article deeply analyzes causes, impacts, and trends, providing valuable insights\n",
    "- Check that the article closely follows the research topic and directly answers questions\n",
    "- Check that the article has a clear structure, fluent language, and is easy to understand.\n",
    "\"\"\"\n",
    "\n",
    "critique_sub_agent = {\n",
    "    \"name\": \"critiquer-agent\",\n",
    "    \"description\": \"Used to critique the final report. Give this agent some information about how you want it to critique the report.\",\n",
    "    \"prompt\": critiquer_prompt,\n",
    "    \"model_settings\": {\n",
    "        \"model\": \"ollama:qwen3:latest\",\n",
    "        \"temperature\":0,\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3958c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai_instructions = \"\"\"\n",
    "You are an expert in the domain of MCP Zero and Alita. Your job is write a comprehensive report through\n",
    "thorough research and analysis.\n",
    "\n",
    "The first thing you should do is to write the original user's question to `question.txt` to keep\n",
    "a record.\n",
    "\n",
    "Then use the alita-mcp-zero-agent to conduct deep research on topics relating to Alita and MCP Zero.\n",
    "\n",
    "When you think you have enough information to write a final report, write it to `final_report.md`.\n",
    "\n",
    "You can call the critiquer-agent to critique the report and suggest improvements. After that (if needed),\n",
    "you can do another round of research with the alita-mcp-zero-agent to improve the report. You can \n",
    "do this however many times you want until you are satisfied with the report.\n",
    "\n",
    "Only edit the file once at a time (if you call this tool in parallel, there may be conflicts).\n",
    "\n",
    "Here are the instructions for writing the final report:\n",
    "\n",
    "<report_instructions>\n",
    "\n",
    "CRITICAL: Make sure the answer is written in the same language as the human messages! If you make a todo plan - you should note in the plan what language the report should be in so you dont forget!\n",
    "Note: the language the report should be in is the language the QUESTION is in, not the language/country that the question is ABOUT.\n",
    "\n",
    "Please create a detailed answer to the overall research brief that:\n",
    "1. Is well-organized with proper headings (# for title, ## for sections, ### for subsections)\n",
    "2. Includes specific facts and insights from the research\n",
    "3. Provides a balanced, thorough analysis. Be as comprehensive as possible, and include all information that is relevant to the overall research question. People are using you for deep research and will expect detailed, comprehensive answers.\n",
    "You can structure your report in a number of different ways. Here are some examples:\n",
    "\n",
    "To answer a question that asks you to compare two things, you might structure your report like this:\n",
    "1/ intro\n",
    "2/ overview of topic A\n",
    "3/ overview of topic B\n",
    "4/ comparison between A and B\n",
    "5/ conclusion\n",
    "\n",
    "To answer a question that asks you to return a list of things, you might only need a single section which is the entire list.\n",
    "1/ list of things or table of things\n",
    "Or, you could choose to make each item in the list a separate section in the report. When asked for lists, you don't need an introduction or conclusion.\n",
    "1/ item 1\n",
    "2/ item 2\n",
    "3/ item 3\n",
    "\n",
    "To answer a question that asks you to summarize a topic, give a report, or give an overview, you might structure your report like this:\n",
    "1/ overview of topic\n",
    "2/ concept 1\n",
    "3/ concept 2\n",
    "4/ concept 3\n",
    "5/ conclusion\n",
    "\n",
    "If you think you can answer the question with a single section, you can do that too!\n",
    "1/ answer\n",
    "\n",
    "REMEMBER: Section is a VERY fluid and loose concept. You can structure your report however you think is best, including in ways that are not listed above!\n",
    "Make sure that your sections are cohesive, and make sense for the reader.\n",
    "\n",
    "For each section of the report, do the following:\n",
    "- Use simple, clear language\n",
    "- Use ## for section title (Markdown format) for each section of the report\n",
    "- Do NOT ever refer to yourself as the writer of the report. This should be a professional report without any self-referential language. \n",
    "- Do not say what you are doing in the report. Just write the report without any commentary from yourself.\n",
    "- Each section should be as long as necessary to deeply answer the question with the information you have gathered. It is expected that sections will be fairly long and verbose. You are writing a deep research report, and users will expect a thorough answer.\n",
    "- Use bullet points to list out information when appropriate, but by default, write in paragraph form.\n",
    "\n",
    "REMEMBER:\n",
    "The brief and research may be in English, but you need to translate this information to the right language when writing the final answer.\n",
    "Make sure the final answer report is in the SAME language as the human messages in the message history.\n",
    "</report_instructions>\n",
    "\n",
    "You have access to a few tools. Namely:\n",
    "## `alita_documentation`\n",
    "## `mcp_zero_documentation`\n",
    "\n",
    "Use these tools to get information about Alita and MCP Zero. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25cf4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_deep_agent = create_deep_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    instructions=\"\"\"\n",
    "    You are an expert in the domain of MCP Zero and Alita. Your job is write a comprehensive report through\n",
    "    thorough research and analysis. \n",
    "    \n",
    "    Here are the instructions for writing the final report:\n",
    "\n",
    "    <report_instructions>\n",
    "\n",
    "    CRITICAL: Make sure the answer is written in the same language as the human messages! If you make a todo plan - you should note in the plan what language the report should be in so you dont forget!\n",
    "    Note: the language the report should be in is the language the QUESTION is in, not the language/country that the question is ABOUT.\n",
    "\n",
    "    Please create a detailed answer to the overall research brief that:\n",
    "    1. Is well-organized with proper headings (# for title, ## for sections, ### for subsections)\n",
    "    2. Includes specific facts and insights from the research\n",
    "    3. Provides a balanced, thorough analysis. Be as comprehensive as possible, and include all information that is relevant to the overall research question. People are using you for deep research and will expect detailed, comprehensive answers.\n",
    "    You can structure your report in a number of different ways. Here are some examples:\n",
    "\n",
    "    To answer a question that asks you to compare two things, you might structure your report like this:\n",
    "    1/ intro\n",
    "    2/ overview of topic A\n",
    "    3/ overview of topic B\n",
    "    4/ comparison between A and B\n",
    "    5/ conclusion\n",
    "\n",
    "    To answer a question that asks you to return a list of things, you might only need a single section which is the entire list.\n",
    "    1/ list of things or table of things\n",
    "    Or, you could choose to make each item in the list a separate section in the report. When asked for lists, you don't need an introduction or conclusion.\n",
    "    1/ item 1\n",
    "    2/ item 2\n",
    "    3/ item 3\n",
    "\n",
    "    To answer a question that asks you to summarize a topic, give a report, or give an overview, you might structure your report like this:\n",
    "    1/ overview of topic\n",
    "    2/ concept 1\n",
    "    3/ concept 2\n",
    "    4/ concept 3\n",
    "    5/ conclusion\n",
    "\n",
    "    If you think you can answer the question with a single section, you can do that too!\n",
    "    1/ answer\n",
    "\n",
    "    REMEMBER: Section is a VERY fluid and loose concept. You can structure your report however you think is best, including in ways that are not listed above!\n",
    "    Make sure that your sections are cohesive, and make sense for the reader.\n",
    "\n",
    "    For each section of the report, do the following:\n",
    "    - Use simple, clear language\n",
    "    - Use ## for section title (Markdown format) for each section of the report\n",
    "    - Do NOT ever refer to yourself as the writer of the report. This should be a professional report without any self-referential language. \n",
    "    - Do not say what you are doing in the report. Just write the report without any commentary from yourself.\n",
    "    - Each section should be as long as necessary to deeply answer the question with the information you have gathered. It is expected that sections will be fairly long and verbose. You are writing a deep research report, and users will expect a thorough answer.\n",
    "    - Use bullet points to list out information when appropriate, but by default, write in paragraph form.\n",
    "\n",
    "    REMEMBER:\n",
    "    The brief and research may be in English, but you need to translate this information to the right language when writing the final answer.\n",
    "    Make sure the final answer report is in the SAME language as the human messages in the message history.\n",
    "    </report_instructions>\n",
    "\n",
    "    You have access to a few tools. Namely:\n",
    "    ## `alita_documentation`\n",
    "    ## `mcp_zero_documentation`\n",
    "\n",
    "    Use these tools to get information about Alita and MCP Zero. \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dfe57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How does Alita enable agents to create and deploy MCP tools if the agent deems that the tool does not exist?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  alita_documentation (c8bdd057-c2c4-46e9-a67e-9f7dc62391b0)\n",
      " Call ID: c8bdd057-c2c4-46e9-a67e-9f7dc62391b0\n",
      "  Args:\n",
      "    query: Alita dynamic tool creation MCP tools agent creates tool if not exist\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: alita_documentation\n",
      "\n",
      "Error: ToolException(\"Error calling tool 'alita_documentation': Unknown index name alita-paper. Please confirm an index with this name exists.\")\n",
      " Please fix your mistakes.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  alita_documentation (8effabdf-6727-4e4b-8825-01b3ab2ef862)\n",
      " Call ID: 8effabdf-6727-4e4b-8825-01b3ab2ef862\n",
      "  Args:\n",
      "    query: Alita dynamic tool creation MCP tools agent creates tool if not exist\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: alita_documentation\n",
      "\n",
      "Error: ToolException(\"Error calling tool 'alita_documentation': Unknown index name alita-paper. Please confirm an index with this name exists.\")\n",
      " Please fix your mistakes.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  alita_documentation (eab34b5b-f903-4cc5-9b3a-19f752bd7d20)\n",
      " Call ID: eab34b5b-f903-4cc5-9b3a-19f752bd7d20\n",
      "  Args:\n",
      "    query: dynamic tool creation Alita agent creates tool if not exist\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: alita_documentation\n",
      "\n",
      "Error: ToolException(\"Error calling tool 'alita_documentation': Unknown index name alita-paper. Please confirm an index with this name exists.\")\n",
      " Please fix your mistakes.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  write_todos (9570760d-f908-4a6b-8f09-4dfa8eba9814)\n",
      " Call ID: 9570760d-f908-4a6b-8f09-4dfa8eba9814\n",
      "  Args:\n",
      "    todos: [{'state': 'pending', 'task': 'Research Alita dynamic tool creation'}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: write_todos\n",
      "\n",
      "Error: 2 validation errors for write_todos\n",
      "todos.0.content\n",
      "  Field required [type=missing, input_value={'state': 'pending', 'tas... dynamic tool creation'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "todos.0.status\n",
      "  Field required [type=missing, input_value={'state': 'pending', 'tas... dynamic tool creation'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      " Please fix your mistakes.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  write_todo? (4f7b39e2-f926-4c6d-b090-6833d5b99e56)\n",
      " Call ID: 4f7b39e2-f926-4c6d-b090-6833d5b99e56\n",
      "  Args:\n",
      "    todos: [{'content': 'Research Alita dynamic tool creation', 'status': 'pending'}]\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: write_todo?\n",
      "\n",
      "Error: write_todo? is not a valid tool, try one of [write_todos, write_file, read_file, ls, edit_file, alita_documentation, mcp_zero_documentation, task].\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m base_deep_agent.astream(\n\u001b[32m      2\u001b[39m     {\n\u001b[32m      3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m      4\u001b[39m             {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mHow does Alita enable agents to create and deploy MCP tools if the agent deems that the tool does not exist?\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m      5\u001b[39m         ]\n\u001b[32m      6\u001b[39m     },\n\u001b[32m      7\u001b[39m     stream_mode=\u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m ):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m chunk:\n\u001b[32m     10\u001b[39m         chunk[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m].pretty_print()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2939\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2937\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2938\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2939\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2940\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2941\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2942\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2943\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2944\u001b[39m ):\n\u001b[32m   2945\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2946\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2947\u001b[39m         stream_mode,\n\u001b[32m   2948\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2951\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2952\u001b[39m     ):\n\u001b[32m   2953\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:295\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    293\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    296\u001b[39m         t,\n\u001b[32m    297\u001b[39m         retry_policy,\n\u001b[32m    298\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    299\u001b[39m         configurable={\n\u001b[32m    300\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    301\u001b[39m                 _acall,\n\u001b[32m    302\u001b[39m                 weakref.ref(t),\n\u001b[32m    303\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    304\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    305\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    306\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    307\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    308\u001b[39m                 loop=loop,\n\u001b[32m    309\u001b[39m             ),\n\u001b[32m    310\u001b[39m         },\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:706\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    704\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    707\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    708\u001b[39m         )\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    710\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/futures.py:287\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    286\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/futures.py:198\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == _CANCELLED:\n\u001b[32m    197\u001b[39m     exc = \u001b[38;5;28mself\u001b[39m._make_cancelled_error()\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state != _FINISHED:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.InvalidStateError(\u001b[33m'\u001b[39m\u001b[33mResult is not ready.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:465\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    463\u001b[39m         run = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m         ret = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(coro, context=context)\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    467\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/futures.py:287\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    286\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/futures.py:198\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == _CANCELLED:\n\u001b[32m    197\u001b[39m     exc = \u001b[38;5;28mself\u001b[39m._make_cancelled_error()\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state != _FINISHED:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.InvalidStateError(\u001b[33m'\u001b[39m\u001b[33mResult is not ready.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py:655\u001b[39m, in \u001b[36mcreate_react_agent.<locals>.acall_model\u001b[39m\u001b[34m(state, runtime, config)\u001b[39m\n\u001b[32m    653\u001b[39m     response = cast(AIMessage, \u001b[38;5;28;01mawait\u001b[39;00m dynamic_model.ainvoke(model_input, config))  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     response = cast(AIMessage, \u001b[38;5;28;01mawait\u001b[39;00m static_model.ainvoke(model_input, config))  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m    657\u001b[39m \u001b[38;5;66;03m# add agent name to the AIMessage\u001b[39;00m\n\u001b[32m    658\u001b[39m response.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3091\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3089\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3090\u001b[39m                 part = functools.partial(step.ainvoke, input_, config)\n\u001b[32m-> \u001b[39m\u001b[32m3091\u001b[39m             input_ = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(part(), context, create_task=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3092\u001b[39m     \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3093\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/futures.py:287\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    286\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/futures.py:198\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == _CANCELLED:\n\u001b[32m    197\u001b[39m     exc = \u001b[38;5;28mself\u001b[39m._make_cancelled_error()\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state != _FINISHED:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.InvalidStateError(\u001b[33m'\u001b[39m\u001b[33mResult is not ready.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/tasks.py:316\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    314\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5454\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5447\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5448\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5449\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5452\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5453\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5455\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5456\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5457\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5458\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:405\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    395\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    397\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    402\u001b[39m     **kwargs: Any,\n\u001b[32m    403\u001b[39m ) -> BaseMessage:\n\u001b[32m    404\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    406\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    407\u001b[39m         stop=stop,\n\u001b[32m    408\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    409\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    410\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    411\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    412\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    413\u001b[39m         **kwargs,\n\u001b[32m    414\u001b[39m     )\n\u001b[32m    415\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1017\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1008\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1010\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1014\u001b[39m     **kwargs: Any,\n\u001b[32m   1015\u001b[39m ) -> LLMResult:\n\u001b[32m   1016\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1018\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1019\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:937\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m run_managers = \u001b[38;5;28;01mawait\u001b[39;00m callback_manager.on_chat_model_start(\n\u001b[32m    925\u001b[39m     \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m    926\u001b[39m     messages_to_trace,\n\u001b[32m   (...)\u001b[39m\u001b[32m    931\u001b[39m     run_id=run_id,\n\u001b[32m    932\u001b[39m )\n\u001b[32m    934\u001b[39m input_messages = [\n\u001b[32m    935\u001b[39m     _normalize_messages(message_list) \u001b[38;5;28;01mfor\u001b[39;00m message_list \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[32m    936\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    938\u001b[39m     *[\n\u001b[32m    939\u001b[39m         \u001b[38;5;28mself\u001b[39m._agenerate_with_cache(\n\u001b[32m    940\u001b[39m             m,\n\u001b[32m    941\u001b[39m             stop=stop,\n\u001b[32m    942\u001b[39m             run_manager=run_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    943\u001b[39m             **kwargs,\n\u001b[32m    944\u001b[39m         )\n\u001b[32m    945\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages)\n\u001b[32m    946\u001b[39m     ],\n\u001b[32m    947\u001b[39m     return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    948\u001b[39m )\n\u001b[32m    949\u001b[39m exceptions = []\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1145\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1143\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1144\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1145\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1146\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1147\u001b[39m     )\n\u001b[32m   1148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1149\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/langchain_ollama/chat_models.py:993\u001b[39m, in \u001b[36mChatOllama._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    986\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_agenerate\u001b[39m(\n\u001b[32m    987\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    988\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    991\u001b[39m     **kwargs: Any,\n\u001b[32m    992\u001b[39m ) -> ChatResult:\n\u001b[32m--> \u001b[39m\u001b[32m993\u001b[39m     final_chunk = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._achat_stream_with_aggregation(\n\u001b[32m    994\u001b[39m         messages, stop, run_manager, verbose=\u001b[38;5;28mself\u001b[39m.verbose, **kwargs\n\u001b[32m    995\u001b[39m     )\n\u001b[32m    996\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m    997\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m    998\u001b[39m         message=AIMessage(\n\u001b[32m    999\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m         generation_info=generation_info,\n\u001b[32m   1005\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/langchain_ollama/chat_models.py:780\u001b[39m, in \u001b[36mChatOllama._achat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_achat_stream_with_aggregation\u001b[39m(\n\u001b[32m    772\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    773\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    777\u001b[39m     **kwargs: Any,\n\u001b[32m    778\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    779\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aiterate_over_stream(messages, stop, **kwargs):\n\u001b[32m    781\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m final_chunk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    782\u001b[39m             final_chunk = chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/langchain_ollama/chat_models.py:917\u001b[39m, in \u001b[36mChatOllama._aiterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_aiterate_over_stream\u001b[39m(\n\u001b[32m    911\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    912\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m    913\u001b[39m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    914\u001b[39m     **kwargs: Any,\n\u001b[32m    915\u001b[39m ) -> AsyncIterator[ChatGenerationChunk]:\n\u001b[32m    916\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._acreate_chat_stream(messages, stop, **kwargs):\n\u001b[32m    918\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    919\u001b[39m             content = (\n\u001b[32m    920\u001b[39m                 stream_resp[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    921\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    922\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    923\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/langchain_ollama/chat_models.py:725\u001b[39m, in \u001b[36mChatOllama._acreate_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    722\u001b[39m chat_params = \u001b[38;5;28mself\u001b[39m._chat_params(messages, stop, **kwargs)\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_client.chat(**chat_params):\n\u001b[32m    726\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/ollama/_client.py:684\u001b[39m, in \u001b[36mAsyncClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    681\u001b[39m   \u001b[38;5;28;01mawait\u001b[39;00m e.response.aread()\n\u001b[32m    682\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r.aiter_lines():\n\u001b[32m    685\u001b[39m   part = json.loads(line)\n\u001b[32m    686\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m err := part.get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/httpx/_models.py:1031\u001b[39m, in \u001b[36mResponse.aiter_lines\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1029\u001b[39m decoder = LineDecoder()\n\u001b[32m   1030\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aiter_text():\n\u001b[32m   1032\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m decoder.decode(text):\n\u001b[32m   1033\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m line\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/httpx/_models.py:1018\u001b[39m, in \u001b[36mResponse.aiter_text\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m   1016\u001b[39m chunker = TextChunker(chunk_size=chunk_size)\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m-> \u001b[39m\u001b[32m1018\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m byte_content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aiter_bytes():\n\u001b[32m   1019\u001b[39m         text_content = decoder.decode(byte_content)\n\u001b[32m   1020\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker.decode(text_content):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/httpx/_models.py:997\u001b[39m, in \u001b[36mResponse.aiter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    995\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aiter_raw():\n\u001b[32m    998\u001b[39m         decoded = decoder.decode(raw_bytes)\n\u001b[32m    999\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker.decode(decoded):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/httpx/_models.py:1055\u001b[39m, in \u001b[36mResponse.aiter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m   1052\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n\u001b[32m   1056\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_bytes_downloaded += \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[32m   1057\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker.decode(raw_stream_bytes):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/httpx/_client.py:176\u001b[39m, in \u001b[36mBoundAsyncStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream:\n\u001b[32m    177\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:271\u001b[39m, in \u001b[36mAsyncResponseStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._httpcore_stream:\n\u001b[32m    272\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aclose()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream:\n\u001b[32m    404\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m AsyncShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aclose()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection._receive_response_body(**kwargs):\n\u001b[32m    335\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:203\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:35\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream.receive(max_bytes=max_bytes)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio.EndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1254\u001b[39m, in \u001b[36mSocketStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1249\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.is_set()\n\u001b[32m   1250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing()\n\u001b[32m   1251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.is_at_eof\n\u001b[32m   1252\u001b[39m ):\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.resume_reading()\n\u001b[32m-> \u001b[39m\u001b[32m1254\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.wait()\n\u001b[32m   1255\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.pause_reading()\n\u001b[32m   1256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/locks.py:212\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/futures.py:287\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    286\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4860bc23",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'StructuredTool'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m agent = \u001b[43mcreate_deep_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenai_instructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubagents\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43malita_mcp_zero_sub_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritique_sub_agent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m.with_config({\u001b[33m\"\u001b[39m\u001b[33mrecursion_limit\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1000\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/deepagents/graph.py:57\u001b[39m, in \u001b[36mcreate_deep_agent\u001b[39m\u001b[34m(tools, instructions, model, subagents, state_schema)\u001b[39m\n\u001b[32m     55\u001b[39m     model = get_default_model()\n\u001b[32m     56\u001b[39m state_schema = state_schema \u001b[38;5;129;01mor\u001b[39;00m DeepAgentState\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m task_tool = \u001b[43m_create_task_tool\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuilt_in_tools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubagents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_schema\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m all_tools = built_in_tools + \u001b[38;5;28mlist\u001b[39m(tools) + [task_tool]\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m create_react_agent(\n\u001b[32m     66\u001b[39m     model,\n\u001b[32m     67\u001b[39m     prompt=prompt,\n\u001b[32m     68\u001b[39m     tools=all_tools,\n\u001b[32m     69\u001b[39m     state_schema=state_schema,\n\u001b[32m     70\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/deep-agents-playground/.venv/lib/python3.12/site-packages/deepagents/sub_agent.py:32\u001b[39m, in \u001b[36m_create_task_tool\u001b[39m\u001b[34m(tools, instructions, subagents, model, state_schema)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _agent \u001b[38;5;129;01min\u001b[39;00m subagents:\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m _agent:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m         _tools = [\u001b[43mtools_by_name\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m _agent[\u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     34\u001b[39m         _tools = tools\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'StructuredTool'"
     ]
    }
   ],
   "source": [
    "agent = create_deep_agent(\n",
    "    model = model,\n",
    "    tools = tools,\n",
    "    instructions = genai_instructions,\n",
    "    subagents = [alita_mcp_zero_sub_agent, critique_sub_agent],\n",
    ").with_config({\"recursion_limit\": 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a67d9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-agents-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
