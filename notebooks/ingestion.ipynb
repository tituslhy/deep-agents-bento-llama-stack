{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0dbdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315c9c6b",
   "metadata": {},
   "source": [
    "## Ingesting documents into Llama Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54286449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "\n",
    "LLAMA_CLOUD_API_KEY = os.environ['LLAMA_CLOUD_API_KEY']\n",
    "\n",
    "kwargs = {\n",
    "    'dense_similarity_top_k': 10,\n",
    "    'sparse_similarity_top_k': 20,\n",
    "    'enable_reranking': True,\n",
    "    'alpha': 0.5,\n",
    "    'rerank_top_n': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alita_index = LlamaCloudIndex(\n",
    "  name=\"alita-paper\",\n",
    "  project_name=\"Default\",\n",
    "  organization_id=\"bf9b425c-54cb-4182-a93f-8ac6aed04348\",\n",
    "  api_key=LLAMA_CLOUD_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dc8b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_zero_index = LlamaCloudIndex(\n",
    "  name=\"mcp-zero-paper\",\n",
    "  project_name=\"Default\",\n",
    "  organization_id=\"bf9b425c-54cb-4182-a93f-8ac6aed04348\",\n",
    "  api_key=LLAMA_CLOUD_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2cc41bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"gpt-oss:20b\", request_timeout = 600)\n",
    "mcp_zero_engine = mcp_zero_index.as_query_engine(llm=llm, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a466aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = mcp_zero_engine.query(\"What is MCP zero and how does it work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "173bd19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**MCP‑Zero** is an agent framework that lets a large language model (LLM) discover and call external tools on its own instead of being handed a huge list of tool descriptions up front.  \n",
       "It addresses two common bottlenecks in today’s tool‑augmented LLMs:\n",
       "\n",
       "| Problem | Conventional approach | MCP‑Zero solution |\n",
       "|---------|-----------------------|-------------------|\n",
       "| **Context bloat** – all JSON‑schema definitions are injected, consuming tens of thousands of tokens | Inject the whole tool ecosystem into the prompt | The LLM requests only the tools it needs, so only a few relevant schemas are added |\n",
       "| **Passive selection** – the model simply picks from a pre‑selected set | A single query is matched to the whole tool set | The model actively asks for a tool, can refine its request, and can request new tools in later turns |\n",
       "\n",
       "### Core ideas\n",
       "\n",
       "1. **Active Tool Request**  \n",
       "   The LLM emits a tiny, structured block that states what it needs.  \n",
       "   ```xml\n",
       "   <tool assistant>\n",
       "   server: <domain or permission>\n",
       "   tool:   <operation type + target>\n",
       "   </tool assistant>\n",
       "   ```\n",
       "   Because the request is generated by the model itself, it matches the semantics of the tool documentation more closely than a raw user query.\n",
       "\n",
       "2. **Hierarchical Semantic Routing**  \n",
       "   The request is handled in two stages using semantic embeddings:\n",
       "\n",
       "   * **Server filtering** – match the `server` field against short server descriptions (often just one sentence).  \n",
       "   * **Tool ranking** – within each chosen server, rank tools by similarity between the `tool` field and the tool’s description.  \n",
       "   A combined score (product × max of the two similarities) selects the top‑k schemas to feed back to the LLM.\n",
       "\n",
       "3. **Iterative Capability Extension**  \n",
       "   After a tool is used, the LLM checks whether the task is still incomplete.  \n",
       "   If more capability is required, it emits another request; if not, it proceeds.  \n",
       "   This loop allows the agent to build a cross‑domain chain of tools (e.g., filesystem access → code editing → command execution) while never loading the full tool collection into the prompt.\n",
       "\n",
       "### How it works in practice\n",
       "\n",
       "1. **User asks**: “Debug my code in `src/train.py`.”  \n",
       "2. **LLM** → notices it lacks filesystem, code‑analysis, and shell execution tools.  \n",
       "3. **LLM** emits a request for a filesystem read tool.  \n",
       "4. **System** finds the best matching server (e.g., “File System”) and the specific read tool, returns the JSON‑schema.  \n",
       "5. **LLM** calls the tool, gets the file, then requests a code‑analysis tool, and so on.  \n",
       "6. **Once all needed tools are called**, the LLM solves the problem and produces the final answer.\n",
       "\n",
       "Because only the schemas of the actually used tools are sent back, the prompt stays tiny (often a few hundred tokens) even when the total tool ecosystem contains thousands of APIs. The agent remains fully autonomous: it decides *when* and *what* to request, can refine its requests in subsequent turns, and can grow a customized toolchain on the fly."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65bac49",
   "metadata": {},
   "source": [
    "## Composite retrieval\n",
    "Not recommended. It's better to break the question into sub parts and query the correct index with each part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "643b2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cloud import CompositeRetrievalMode, RetrieverPipeline\n",
    "from llama_index.indices.managed.llama_cloud import (\n",
    "    LlamaCloudIndex,\n",
    "    LlamaCloudCompositeRetriever,\n",
    ")\n",
    "\n",
    "retriever = LlamaCloudCompositeRetriever(\n",
    "    name=\"Alita and MCP Zero Retriever\",\n",
    "    api_key=LLAMA_CLOUD_API_KEY,\n",
    "    create_if_not_exists=True,\n",
    "    mode=CompositeRetrievalMode.FULL,\n",
    "    rerank_top_n=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c832a872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Retriever(name='Alita and MCP Zero Retriever', pipelines=[RetrieverPipeline(name='alita-paper', description='Knowledge base for the Alita paradigm for agents', pipeline_id='6e287db8-a658-48c2-837f-1e13c85edc84', preset_retrieval_parameters=PresetRetrievalParams(dense_similarity_top_k=30, dense_similarity_cutoff=0.0, sparse_similarity_top_k=30, enable_reranking=True, rerank_top_n=6, alpha=0.5, search_filters=None, search_filters_inference_schema=None, files_top_k=1, retrieval_mode=<RetrievalMode.CHUNKS: 'chunks'>, retrieve_image_nodes=False, retrieve_page_screenshot_nodes=False, retrieve_page_figure_nodes=False, class_name='base_component')), RetrieverPipeline(name='mcp-zero-paper', description='Knowledge base of the (model context protocol) MCP zero paradigm', pipeline_id='1d5c48e0-9849-49a6-a59d-0af4eb09f794', preset_retrieval_parameters=PresetRetrievalParams(dense_similarity_top_k=30, dense_similarity_cutoff=0.0, sparse_similarity_top_k=30, enable_reranking=True, rerank_top_n=6, alpha=0.5, search_filters=None, search_filters_inference_schema=None, files_top_k=1, retrieval_mode=<RetrievalMode.CHUNKS: 'chunks'>, retrieve_image_nodes=False, retrieve_page_screenshot_nodes=False, retrieve_page_figure_nodes=False, class_name='base_component'))], id='d2c4add1-f4cb-41a0-bcfd-58dac5293769', created_at=datetime.datetime(2025, 8, 13, 2, 38, 0, 856052, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2025, 8, 13, 2, 39, 23, 685850, tzinfo=datetime.timezone.utc), project_id='6bdb9346-fa87-4a33-a621-f595ccbb5986')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.add_index(\n",
    "    alita_index, description=\"Knowledge base for the Alita paradigm for agents\"\n",
    ")\n",
    "retriever.add_index(\n",
    "    mcp_zero_index, description=\"Knowledge base of the (model context protocol) MCP zero paradigm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "863cfa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = retriever.retrieve(\n",
    "    \"What is Alita and what is MCP Zero? Can Alita and MCP zero work together?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5ce16e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='014911b9-3bda-45c1-901f-2c96a94b2295', embedding=None, metadata={'id': 'mcp_zero_paper.pdf', 'file_size': 975244, 'last_modified_at': '2025-08-13T02:23:02', 'file_path': 'mcp_zero_paper.pdf', 'file_name': 'mcp_zero_paper.pdf', 'external_file_id': 'mcp_zero_paper.pdf', 'file_id': '99275b01-66e2-4f31-9b1c-914c3916ba5d', 'pipeline_file_id': '1c2460ac-3fe0-4394-ace3-d4b2df4559e3', 'pipeline_id': '1d5c48e0-9849-49a6-a59d-0af4eb09f794', 'page_label': 10, 'start_page_index': 9, 'start_page_label': 10, 'end_page_index': 9, 'end_page_label': 10, 'document_id': '2e13a37a379d317a1af0a61b9d831caed3d655a04d32d55bc9', 'start_char_idx': 81283, 'end_char_idx': 86577, 'retriever_id': 'd2c4add1-f4cb-41a0-bcfd-58dac5293769', 'retriever_pipeline_name': 'mcp-zero-paper'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='**2. Semantic grounding.** The example also clarifies the *meaning* of each field, helping the model understand the specific definitions of MCP server and tool, thereby limiting its expression scope. After seeing this, the model reliably emits phrases such as `filesystem.read` instead of a vague “read the file”, sharply reducing semantic mismatch.\\n\\nIn short, a tiny demonstration patch acts as a *schema anchor*; future work could replace ICL with a short grammar-based decoder rule, but the one-shot approach is free and highly effective.\\n\\n### 7.3. Synergy with Alita: Using *and* Making Tools\\n\\nConcurrently, **Alita** [22] proposes a united manager agent that *creates* its own toolchain: it web-searches for code, clones GitHub repos, builds environments, and executes the resulting programs to accomplish tasks. We were pleasantly surprised by the contribution of this article, and found that the two lines of work are complementary:  \\n* MCP-Zero: *efficiently finds and invokes* existing tools  \\n* Alita: *automatically builds missing tools on-the-fly*\\n\\nMCP-Zero and Alita address complementary halves of the same problem: the former maximises *tool discovery* while the latter maximises *tool creation*. When combined, they form a virtuous loop: an agent first actively discovers tools from *all* available resources; if none fits, it switches to Alita’s workflow to synthesize a new one, then registers the freshly built tool for the community. We believe such a pipeline is a compelling direction toward self-evolving, cost-aware agentic AI systems.\\n\\n### 7.4. Future Work\\n\\nWhile MCP-Zero demonstrates significant improvements in tool retrieval efficiency and accuracy, several promising directions warrant further investigation:\\n\\n**Enhanced Experimental Validation.** Future work should expand evaluation across diverse domains. We plan to conduct comprehensive experiments on additional datasets to validate generalizability.\\n\\n**Advanced Matching Algorithms.** The current semantic similarity approach could be enhanced. We envision incorporating multi-modal descriptions (e.g., code examples, usage patterns, parameter schemas) into the retrieval process, and exploring usage co-occurrence patterns for improved contextual understanding.\\n\\n**MCP Server Implementation.** A natural extension involves packaging MCP-Zero as a dedicated MCP server providing tool discovery services. This ”meta-server” would expose standardized APIs for active tool retrieval, enabling seamless integration into existing MCP ecosystems and serving as a centralized discovery hub for distributed tool collections.\\n\\n**Multi-Agent Orchestration.** MCP-Zero’s active discovery approach could enable better multi-agent collaboration. Future work could investigate how different agents can automatically discover and share tools with each other, allowing them to work together more effectively on complex tasks that require diverse capabilities.\\n\\n----\\n\\n## References\\n\\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. *arXiv preprint arXiv:2303.08774*, 2023. 2\\n\\n[2] Anthropic. Model context protocol. https://docs.anthropic.com/en/docs/agents-and-tools/mcp, 2024. Accessed: June 25, 2025. 4\\n\\n[3] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In *International conference on machine learning*, pages 2206–2240. PMLR, 2022. 3\\n\\n[4] Harrison Chase. Langchain. https://github.com/langchain-ai/langchain, 2022. Python framework for developing applications powered by language models. 3\\n\\n[5] Yanfei Chen, Jinsung Yoon, Devendra Singh Sachan, Qingze Wang, Vincent Cohen-Addad, Mohammadhossein Bateni, Chen-Yu Lee, and Tomas Pfister. Re-invoke: Tool invocation rewriting for zero-shot tool retrieval. *arXiv preprint arXiv:2408.01875*, 2024. 3\\n\\n[6] Yu Du, Fangyun Wei, and Hongyang Zhang. Anytool: Self-reflective, hierarchical agents for large-scale api calls. *arXiv preprint arXiv:2402.04253*, 2024. 3\\n\\n[7] Tiantian Gan and Qiyao Sun. Rag-mcp: Mitigating prompt bloat in llm tool selection via retrieval-augmented generation. *arXiv preprint arXiv:2505.03275*, 2025. 2, 3, 4\\n\\n[8] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. *arXiv preprint arXiv:2407.21783*, 2024. 2\\n\\n[9] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pretraining. In *International conference on machine learning*, pages 3929–3938. PMLR, 2020. 3\\n\\n[10] Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, et al. Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. *arXiv preprint arXiv:2205.00445*, 2022. 3\\n\\n[11] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.81875163),\n",
       " NodeWithScore(node=TextNode(id_='c134ff1b-8bc1-4974-8db3-aeb258edb0cf', embedding=None, metadata={'id': 'mcp_zero_paper.pdf', 'file_size': 975244, 'last_modified_at': '2025-08-13T02:23:02', 'file_path': 'mcp_zero_paper.pdf', 'file_name': 'mcp_zero_paper.pdf', 'external_file_id': 'mcp_zero_paper.pdf', 'file_id': '99275b01-66e2-4f31-9b1c-914c3916ba5d', 'pipeline_file_id': '1c2460ac-3fe0-4394-ace3-d4b2df4559e3', 'pipeline_id': '1d5c48e0-9849-49a6-a59d-0af4eb09f794', 'page_label': 9, 'start_page_index': 8, 'start_page_label': 9, 'end_page_index': 8, 'end_page_label': 9, 'document_id': '2e13a37a379d317a1af0a61b9d831caed3d655a04d32d55bc9', 'start_char_idx': 70935, 'end_char_idx': 76101, 'retriever_id': 'd2c4add1-f4cb-41a0-bcfd-58dac5293769', 'retriever_pipeline_name': 'mcp-zero-paper'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='* **Extreme Context Efficiency.** MCP-Zero cuts prompt length by **60–98%** across all settings (e.g. 111 vs. 6.3k tokens in the full single-turn case), validating its ability to ”pay for tools only when they are needed”.\\n\\n* **Robust Scalability.** When moving from a hand-curated *Domain* subset to the *Full* tool pool (40x more APIs), standard schema-injection accuracy on Claude-3.5 plummets from 97.60 to 69.23 (single-turn) and 100.00 to 60.22 (multi-turn); MCP-Zero instead keeps accuracy at 95.19 / 90.32 respectively, demonstrating strong resilience to attention dilution.\\n\\n* **Multi-turn Consistency.** MCP-Zero maintains high accuracy over conversation rounds (≤3% drop from single- to multi-turn), whereas standard methods degrade sharply once the context accumulates previous calls and larger tool sets.\\n\\n* **Necessity of Active Requests.** Pure query-retrieval baselines stall at 65–72 % accuracy, confirming that letting the model *author* semantically aligned requests is crucial.\\n\\nExperiments on APIBank corroborate our claims: MCP-Zero delivers near-optimal or superior tool-selection accuracy while slashing context usage by up to two orders of magnitude, remaining robust in both single- and multi-turn conversations and under massive tool-pool scaling. These results highlight active, iterative tool discovery as a practical path toward scalable, cost-efficient agent systems.\\n\\n## 6. Conclusion\\n\\nThis work establishes active tool discovery as a fundamental paradigm for autonomous agent systems, enabling models to maintain decision autonomy while addressing critical scalability challenges in tool-calling architectures. MCP-Zero demonstrates that shifting from passive tool consumption to agent-driven capability acquisition achieves substantial efficiency gains—98% token reduction with preserved accuracy—while restoring the core principle of autonomous agency: the ability to assess limitations and actively acquire necessary resources. Our theoretical framework, empirical validation, and the MCP-tools dataset provide both the foundation and infrastructure for advancing autonomous agent architectures as tool ecosystems continue expanding exponentially.\\n\\n## 7. Discussion\\n\\nIn this section we reflect on how the MCP-Zero paradigm can be adopted by other researchers (§7.1), analyse the surprisingly gain from a single in–context example (§7.2), and position MCP-Zero with respect to the contemporaneous *Alita* system, outlining a promising path toward self-improving agentic AI (§7.3).\\n\\n### 7.1. Cookbook: Integrate MCP-Zero Into Agent\\n\\nMCP-Zero is fundamentally a simple yet effective approach that we hope will benefit the broader MCP community. The core methodology distills into three straightforward steps: prompting models to actively request tools, maintaining a lightweight tool index with semantic descriptions, and leveraging the improved semantic alignment for high-precision retrieval. Below we provide a practical guide for integrating these ideas into existing agent frameworks.\\n\\n**Step 1 – Prompting the LLM to ask for tools.**  \\nGive the model an explicit “permission” to declare missing capabilities. In practice this is a `system` instruction such as:\\n\\n```text\\nIf the current task cannot be solved with your\\nown knowledge, emit a <tool assistant> block\\nspecifying the server domain and the tool\\noperation you require.\\n```\\n\\nIn addition, the output structure needs to be specified as we mentioned in Section 3.1. This step aims to stimulate the model’s ability to ”actively” propose requirements.\\n\\n**Step 2 – Curate a lightweight MCP-style tool index.**  \\nFirstly, choose a scope based on your needs: the entire MCP-tools collection, a vertical slice (e.g. databases only), or your in-house APIs. Then, for every server/tool:  \\n* extract the name and description from metadata;  \\n* optionally let a strong LLM generate an *enhanced summary* that emphasises capabilities and usage patterns;  \\n* store all texts in a vector store with pre-computed embeddings such as `text-embedding-3-large`.\\n\\n**Step 3 – Marry model output and retrieval.**  \\nWhen the agent emits a `<tool assistant>` block:  \\n* Match the `server` field against server descriptions and summaries; take top-*m* candidates.  \\n* Within each candidate server, rank tools by the `tool` field with the tool description embeddings.  \\n* Feed the best (or top-*k*) JSON-schemas back to the LLM.\\n\\nBecause the request text is already semantically aligned with the documents, retrieval precision is higher than “user query → API doc” matching, maintaining performances while significantly conserving context.\\n\\n### 7.2. Why Does a Single ICL Example Help?\\n\\nIn §5.2 we observed that adding **one** in-context example (“ICL-1”) helps lifting needle-in-haystack accuracy marginally. We hypothesise two simple but potent effects:  \\n1. **Stylistic anchor.** Our base prompt merely says “output the server and tool you need”, but gives no example of *how* the sentence should look like. The single in-context sample provides the writing style as the reference, helping the generated requests land much closer to the curated descriptions, thus semantic matching becomes easier.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.72768575),\n",
       " NodeWithScore(node=TextNode(id_='fcd6b416-ba60-46d5-aaf0-ecd93807a04e', embedding=None, metadata={'id': 'mcp_zero_paper.pdf', 'file_size': 975244, 'last_modified_at': '2025-08-13T02:23:02', 'file_path': 'mcp_zero_paper.pdf', 'file_name': 'mcp_zero_paper.pdf', 'external_file_id': 'mcp_zero_paper.pdf', 'file_id': '99275b01-66e2-4f31-9b1c-914c3916ba5d', 'pipeline_file_id': '1c2460ac-3fe0-4394-ace3-d4b2df4559e3', 'pipeline_id': '1d5c48e0-9849-49a6-a59d-0af4eb09f794', 'page_label': 11, 'start_page_index': 10, 'start_page_label': 11, 'end_page_index': 10, 'end_page_label': 11, 'document_id': '2e13a37a379d317a1af0a61b9d831caed3d655a04d32d55bc9', 'start_char_idx': 91887, 'end_char_idx': 97271, 'retriever_id': 'd2c4add1-f4cb-41a0-bcfd-58dac5293769', 'retriever_pipeline_name': 'mcp-zero-paper'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. *Advances in neural information processing systems*, 33:9459–9474, 2020. 3\\n\\n[12] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms. *arXiv preprint arXiv:2304.08244*, 2023. 8\\n\\n[13] Zhiling Luo, Xiaorong Shi, Xuanrui Lin, and Jinyang Gao. Evaluation report on mcp servers. *arXiv preprint arXiv:2504.11094*, 2025. 3, 6, 7\\n\\n[14] Tula Masterman, Sandi Besen, Mason Sawtell, and Alex Chao. The landscape of emerging ai agent architectures for reasoning, planning, and tool calling: A survey. *arXiv preprint arXiv:2404.11584*, 2024. 2\\n\\n[15] Suhong Moon, Siddharth Jha, Lutfi Eren Erdogan, Sehoon Kim, Woosang Lim, Kurt Keutzer, and Amir Gholami. Efficient and scalable estimation of tool representations in vector space. *arXiv preprint arXiv:2409.02141*, 2024. 2, 3, 8\\n\\n[16] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. *arXiv preprint arXiv:2112.09332*, 2021. 3\\n\\n[17] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. *arXiv preprint arXiv:2303.09014*, 2023. 3\\n\\n[18] Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model connected with massive apis. *Advances in Neural Information Processing Systems*, 37:126544–126565, 2024. 2, 3, 8\\n\\n[19] Yun Peng, Shuqing Li, Wenwei Gu, Yichen Li, Wenxuan Wang, Cuiyun Gao, and Michael R Lyu. Revisiting, benchmarking and exploring api recommendation: How far are we? *IEEE Transactions on Software Engineering*, 49(4):1876–1897, 2022. 8\\n\\n[20] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. *arXiv preprint arXiv:2307.16789*, 2023. 8\\n\\n[21] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Xuanhe Zhou, Yufei Huang, Chaojun Xiao, et al. Tool learning with foundation models. *ACM Computing Surveys*, 57(4):1–40, 2024. 2\\n\\n[22] Jiahao Qiu, Xuan Qi, Tongcheng Zhang, Xinzhe Juan, Jiacheng Guo, Yifu Lu, Yimin Wang, Zixin Yao, Qihan Ren, Xun Jiang, et al. Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution. *arXiv preprint arXiv:2505.20286*, 2025. 10\\n\\n[23] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. Colt: Towards completeness-oriented tool retrieval for large language models. *arXiv e-prints*, pages arXiv–2405, 2024. 3\\n\\n[24] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. 7\\n\\n[25] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. *Transactions of the Association for Computational Linguistics*, 11:1316–1331, 2023. 3\\n\\n[26] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. *Advances in Neural Information Processing Systems*, 36:68539–68551, 2023. 2, 3\\n\\n[27] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. *Advances in Neural Information Processing Systems*, 36:38154–38180, 2023. 3\\n\\n[28] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. *arXiv preprint arXiv:2306.05301*, 2023. 8\\n\\n[29] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversation. *arXiv preprint arXiv:2308.08155*, 2023. 3\\n\\n[30] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. *arXiv preprint arXiv:2505.09388*, 2025. 2\\n\\n[31] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In *International Conference on Learning Representations (ICLR)*, 2023. 3\\n\\n[32] Yuanhang Zheng, Peng Li, Wei Liu, Yang Liu, Jian Luan, and Bin Wang. Toolrerank: Adaptive and hierarchy-aware reranking for tool retrieval. *arXiv preprint arXiv:2403.06551*, 2024. 3', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6213807),\n",
       " NodeWithScore(node=TextNode(id_='da02f16e-705e-4a5b-bacf-fc38bbe5369e', embedding=None, metadata={'id': 'alita_paper.pdf', 'file_size': 1113373, 'last_modified_at': '2025-08-13T02:06:25', 'file_path': 'alita_paper.pdf', 'file_name': 'alita_paper.pdf', 'external_file_id': 'alita_paper.pdf', 'file_id': 'bcee7eb9-6479-46a6-9122-fd5b454699ef', 'pipeline_file_id': '7f9959a9-84df-4234-b475-e11e1bda3db1', 'pipeline_id': '6e287db8-a658-48c2-837f-1e13c85edc84', 'page_label': 3, 'start_page_index': 2, 'start_page_label': 3, 'end_page_index': 2, 'end_page_label': 3, 'document_id': 'c9f76edb0a0d2f097f23ba53edef10f99a6924dfccb183a929', 'start_char_idx': 16469, 'end_char_idx': 21079, 'retriever_id': 'd2c4add1-f4cb-41a0-bcfd-58dac5293769', 'retriever_pipeline_name': 'alita-paper'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='* We present Alita, a generalist agent that achieves scalable agentic reasoning with a radically simple design.  \\n* We empirically demonstrate that Alita, despite using no complex predefined tools and workflows for specific tasks, outperforms many systems with significantly more handcrafted complexity on the GAIA benchmark. We achieve 75.15% pass@1 and 87.27% pass@3, surpassing OpenAI’s Deep Research with 67.36% pass@1 and ranking top among all general-purpose agents.\\n\\n## 2 Related Works\\n\\n### 2.1 Generalist Agent\\n\\nThe concept of a Generalist Agent aims to construct an AI agent system capable of collaboratively completing a variety of complex tasks in a real-world environment. OWL [8] introduces a method that decomposes complex tasks into subtasks and dynamically allocates them to worker nodes with specialized toolkits. Omne [11] proposes a multi-agent collaborative development framework, where each agent possesses an independent system structure, enabling autonomous learning and the storage of a comprehensive world model to build an independent understanding of the environment. OpenAI Deep Research[^2] employs reinforcement learning for training on real-world tasks, aiming to provide precise and reliable research reports for knowledge-intensive tasks. A-World [12] offers a highly configurable, modular, and scalable simulation environment, allowing developers to flexibly define and integrate different types of AI agents. The Magentic-One [13] framework merges the Magentic and Autogen systems, distinguishing between the micro-level LLM-driven function generation and the macro-level multi-agent orchestration, resulting in a clearer and more efficient construction of agent systems. Alita, also a Generalist Agent, allows for the minimal use of predefined tools and workflows for direct problem-solving, yet still achieves impressive performance across diverse tasks.\\n\\n### 2.2 Auto Generating Agent\\n\\nAuto-generating agents aim to enhance the versatility of agents by enabling them to autonomously generate tools, agents, or workflows tailored to specific tasks. AutoAgents [14], for instance, generate multiple agents, each playing a distinct role, to handle the corresponding subtasks. OpenHands [15] offers an event-driven architecture that allows agents to interact with the environment like human developers, thereby enabling the creation of custom workflows. AFlow [16] redefines workflow optimization as a search problem, where each workflow consists of several nodes invoking large LLMs, and the optimal workflow is identified and executed through a search process. AutoAgent [17], as an autonomous agent operating system, permits agents to manage system-level operations and file data autonomously. In Alita, agents are empowered to automatically generate diverse, specialized, and highly accurate MCPs to aid in the completion of specific tasks, while also providing resources for future executions.\\n\\n### 2.3 Tool Creation\\n\\nTool Creation enables agents to autonomously create tools to assist in task execution, either on their own or with external support. CRAFT [18] utilizes GPT-4 to generate a set of code snippets that function as tools, which are then retrieved and used in the system. TroVE [19] maintains a collection of high-level functions, which are automatically generated, extended, and periodically pruned to optimize program generation. CREATOR [20] decouples the abstraction of tool creation from the actual execution, allowing LLMs to address tasks at different levels of granularity. AutoAgent [17] enables agents to autonomously create new tools based on task requirements, incorporating information gathered through web searches and integrating these tools into their workflow. OpenHands [15] allows agents to create code scripts in a human-like manner during interaction with the environment to assist in task completion. In comparison, Alita enables MCP creation, which provides additional benefits, including better reusability and easy environment management over tool creation.\\n\\n### 2.4 MCP\\n\\nThe Model Context Protocol (MCP) is a standard proposed by Anthropic, designed to unify the connection between AI systems and external data sources and services. RAG-MCP [21] enhances the efficiency and accuracy of agents by retrieving the most relevant tools from a large collection, based on the task description, within the database composed of MCP descriptions. After tool generation, Alita wraps the generated valid tools into MCPs for subsequent use, facilitating reuse by itself and other agents.\\n\\n[^2]: https://openai.com/index/introducing-deep-research/', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.5971152),\n",
       " NodeWithScore(node=TextNode(id_='33d22e1b-3cc9-4e47-afc3-130ff82a540e', embedding=None, metadata={'id': 'alita_paper.pdf', 'file_size': 1113373, 'last_modified_at': '2025-08-13T02:06:25', 'file_path': 'alita_paper.pdf', 'file_name': 'alita_paper.pdf', 'external_file_id': 'alita_paper.pdf', 'file_id': 'bcee7eb9-6479-46a6-9122-fd5b454699ef', 'pipeline_file_id': '7f9959a9-84df-4234-b475-e11e1bda3db1', 'pipeline_id': '6e287db8-a658-48c2-837f-1e13c85edc84', 'page_label': 2, 'start_page_index': 1, 'start_page_label': 2, 'end_page_index': 1, 'end_page_label': 2, 'document_id': 'c9f76edb0a0d2f097f23ba53edef10f99a6924dfccb183a929', 'start_char_idx': 5429, 'end_char_idx': 10941, 'retriever_id': 'd2c4add1-f4cb-41a0-bcfd-58dac5293769', 'retriever_pipeline_name': 'alita-paper'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='# 1 Introduction\\n\\n> \"Simplicity is the ultimate sophistication.\"  \\n> — Leonardo da Vinci\\n\\nLarge language models (LLMs) have rapidly evolved from merely generating text to autonomous agents capable of independently planning and executing complex tasks on behalf of users with limited human oversight [2]. These capabilities have enabled a wide range of applications, ranging from travel planning [3], computer use [4, 5, 6], to the multi-step research tasks [7]. To support such diverse and demanding tasks, a new class of systems called generalist agents has emerged. These agents are designed to handle a wide range of domains and tasks through a unified architecture, allowing them to generalize beyond task-specific solutions, such as OpenAI Deep Research [1] and Manus.\\n\\nHowever, most of the current general-purpose agents heavily rely on large-scale manual engineering, including tediously designed workflows, considerable pre-defined tools, and hardcoded components [8, 9]. This reliance introduces several critical limitations:  \\ni) It is impractical, if not impossible, to predefine all the tools required for the wide variety of real-world tasks an agent might encounter (*incomplete coverage*);  \\nii) Many complex tasks require agents to creatively compose new tools or leverage existing ones in novel ways while pre-designed workflow and hardcoded components constrain this compositional flexibility and inhibit the development of adaptive behaviors (*limited creativity and flexibility*);  \\niii) It is not always the case that the interface or environment of different tools are compatible with the agent (*mismatch*). For example, many useful tools are not written in Python, which makes it difficult, though not entirely impossible, for them to be pre-connected to the mainstream agent frameworks that are primarily written in Python. Together, these challenges ultimately hinder the scalability, adaptability, and generalization of existing generalist agents.\\n\\nIn contrast to the prevailing trend of growing complexity, we propose a radically simple design philosophy built on two principles:  \\ni) *Minimal Predefinition*: Equip the agent with only a minimal set of core capabilities, avoiding manually engineered components for specific tasks or modalities;  \\nii) *Maximal Self-Evolution*: Empower the agent to autonomously create, refine, and reuse external capabilities as needed. We instantiate this vision through Alita, a generalist agent built with a single core capability (i.e., the web agent) and a small set of general-purpose modules that enable self-directed capability expansion. Specifically, we take advantage of the Model Context Protocols (MCPs) [1] which is an open protocol that standardizes how different systems provide context to LLMs, and empower Alita to dynamically generate, adapt, and reuse MCPs based on the demands of each task rather than relying on static, predefined tools. This shift from manually designed capabilities to on-the-fly MCP construction unlocks a new path for building agents that are simple yet profoundly capable.\\n\\n<table>\\n<thead>\\n<tr>\\n  <th colspan=\"6\">Traditional Generalist Agents</th>\\n  <th></th>\\n  <th colspan=\"3\">Large-scale Manual Engineering</th>\\n  <th></th>\\n  <th colspan=\"3\">Alita (Ours)</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n  <td>Url Text Extractor</td>\\n  <td>URL TEXT</td>\\n  <td>Image Captioner</td>\\n  <td>Relevant Patch Zoomer</td>\\n  <td>Other Agents</td>\\n  <td>Youtube Caption Crawler</td>\\n  <td></td>\\n  <td>Incomplete Coverage</td>\\n  <td>Limited Creativity & Flexibility</td>\\n  <td>Mismatch</td>\\n  <td></td>\\n  <td>Web Agent</td>\\n  <td>Manager Agent</td>\\n  <td>MCP Creation</td>\\n</tr>\\n<tr>\\n  <td>Web Agent</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>Self Evolving</td>\\n  <td>MCP Box</td>\\n</tr>\\n<tr>\\n  <td>Path Generalist Classifier</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>Manager Agent</td>\\n  <td></td>\\n  <td></td>\\n  <td>Minimal Predefinition</td>\\n  <td></td>\\n  <td>Maximal Self-Evolution</td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>SELF-EVOLUTION</td>\\n  <td></td>\\n</tr>\\n<tr>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td></td>\\n  <td>Scalable Dynamic Capability</td>\\n  <td>Enhanced Creativity & Flexibility</td>\\n  <td colspan=\"2\">Cross-ecosystem Compatibility</td>\\n</tr>\\n</tbody>\\n</table>\\n\\n**Figure 2:** Comparison between Traditional Generalist Agents and Alita. Traditional generalist agents heavily rely on large-scale manual engineering while Alita adheres to minimal predefinition and maximal self-evolution.\\n\\nWe conduct comprehensive experiments on several benchmarks to assess Alita in real-world applications, especially on the most popular GAIA [10]. Alita proves that simplicity is not a constraint, but a strength, and that creative agent behavior can emerge from a design that prioritizes autonomy over manual engineering. To sum up, our key contributions can be summarized as follows.\\n\\n* We propose a new agent architecture centered on minimal predefinition and maximal self-evolution, challenging conventional design norms in generalist agents, aiming to call for a more scalable and generalizable agent framework.\\n\\n----\\n\\n[1]: https://www.anthropic.com/news/model-context-protocol', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.54594),\n",
       " NodeWithScore(node=TextNode(id_='0a8ced32-cc9c-4e43-912d-c155d50d4fc3', embedding=None, metadata={'id': 'alita_paper.pdf', 'file_size': 1113373, 'last_modified_at': '2025-08-13T02:06:25', 'file_path': 'alita_paper.pdf', 'file_name': 'alita_paper.pdf', 'external_file_id': 'alita_paper.pdf', 'file_id': 'bcee7eb9-6479-46a6-9122-fd5b454699ef', 'pipeline_file_id': '7f9959a9-84df-4234-b475-e11e1bda3db1', 'pipeline_id': '6e287db8-a658-48c2-837f-1e13c85edc84', 'page_label': 1, 'start_page_index': 0, 'start_page_label': 1, 'end_page_index': 0, 'end_page_label': 1, 'document_id': 'c9f76edb0a0d2f097f23ba53edef10f99a6924dfccb183a929', 'start_char_idx': 1, 'end_char_idx': 2707, 'retriever_id': 'd2c4add1-f4cb-41a0-bcfd-58dac5293769', 'retriever_pipeline_name': 'alita-paper'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='# ALITA: GENERALIST AGENT ENABLING SCALABLE AGENTIC REASONING WITH MINIMAL PREDEFINITION AND MAXIMAL SELF-EVOLUTION\\n\\nJiahao Qiu*¹, Xuan Qi*², Tongcheng Zhang*³, Xinzhe Juan³⁴, Jiacheng Guo¹, Yifu Lu¹, Yimin Wang³⁴, Zixin Yao¹,  \\nQihan Ren³, Xun Jiang⁵, Xing Zhou⁵, Dongrui Liu³, Ling Yang¹, Yue Wu¹, Kaixuan Huang¹, Shilong Liu¹,  \\nHongru Wang⁶, Mengdi Wang¹\\n\\n¹ AI Lab, Princeton University  \\n² IIIS, Tsinghua University  \\n³ Shanghai Jiao Tong University  \\n⁴ University of Michigan  \\n⁵ Tianqiao and Chrissy Chen Institute  \\n⁶ The Chinese University of Hong Kong\\n\\n<table>\\n<thead>\\n<tr>\\n<th colspan=\"4\">GAIA Benchmark</th>\\n</tr>\\n<tr>\\n<th></th>\\n<th>Alita</th>\\n<th>manus.ai</th>\\n<th>OpenAI DeepResearch</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>Level 1</td>\\n<td>88.7%</td>\\n<td>86.5%</td>\\n<td>74.3%</td>\\n</tr>\\n<tr>\\n<td>Level 2</td>\\n<td>89.5%</td>\\n<td>70.1%</td>\\n<td>69.1%</td>\\n</tr>\\n<tr>\\n<td>Level 3</td>\\n<td>76.9%</td>\\n<td>57.7%</td>\\n<td>47.6%</td>\\n</tr>\\n<tr>\\n<td>Average</td>\\n<td>87.3%</td>\\n<td>73.3%</td>\\n<td>67.4%</td>\\n</tr>\\n</tbody>\\n</table>\\n\\n> Figure 1: Performance of Alita, manus.ai, and OpenAI DeepResearch[1]\\n\\n## ABSTRACT\\n\\nRecent advances in large language models (LLMs) have enabled agents to autonomously perform complex, open-ended tasks. However, many existing frameworks depend heavily on manually predefined tools and workflows, which hinder their adaptability, scalability, and generalization across domains. In this work, we introduce **Alita**—a generalist agent designed with the principle of *\"Simplicity is the ultimate sophistication,\"* enabling scalable agentic reasoning through **minimal predefinition** and **maximal self-evolution**. \\n\\nFor minimal predefinition, Alita is equipped with only one component for direct problem-solving, making it much simpler and neater than previous approaches that relied heavily on hand-crafted, elaborate tools and workflows. This clean design enhances its potential to generalize to challenging questions, without being limited by tools. \\n\\nFor *Maximal self-evolution*, we enable the creativity of Alita by providing a suite of general-purpose components to autonomously construct, refine, and reuse external capabilities by generating task-related model context protocols (MCPs) from open source, which contributes to scalable agentic reasoning. \\n\\nNotably, Alita achieves 75.15% pass@1 and 87.27% pass@3 accuracy, which is top-ranking among general-purpose agents, on the GAIA benchmark validation dataset, 74.00% and 52.00% pass@1, respectively, on Mathvista and PathVQA, outperforming many agent systems with far greater complexity. More details will be updated at https://github.com/CharlesQ9/Alita.\\n\\n\\\\* These authors contributed equally to this work.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.5383771)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c38ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-agents-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
